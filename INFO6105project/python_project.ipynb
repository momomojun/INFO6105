{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b407948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 733 entries, 0 to 732\n",
      "Data columns (total 7 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               733 non-null    int64  \n",
      " 1   Glucose                   633 non-null    float64\n",
      " 2   BloodPressure             733 non-null    int64  \n",
      " 3   SkinThickness             543 non-null    float64\n",
      " 4   BMI                       726 non-null    float64\n",
      " 5   DiabetesPedigreeFunction  689 non-null    float64\n",
      " 6   Age                       733 non-null    int64  \n",
      "dtypes: float64(4), int64(3)\n",
      "memory usage: 40.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "\n",
    "# Developing a semi-supervised learning model on Diabetes dataset \n",
    "# using a super learner and deploying the model on other datasets.\n",
    "\n",
    "# Step 1: Data pre-processing phase\n",
    "\n",
    "# Remove outliers from all columns.\n",
    "# Impute missing values in all columns.\n",
    "# Normalize all columns.\n",
    "\n",
    "# Step 2: Unsupervised Learning for generating labels\n",
    "\n",
    "# Use K-means clustering on three features of Glucose, \n",
    "# BMI and Age to cluster data into two clusters.\n",
    "# Assign ‘Diabetes’ name to the cluster with higher average Glucose \n",
    "# and ‘No Diabetes’ to the other cluster.\n",
    "# Add a new column (Outcome) to the dataset containing 1 for ‘Diabetes’ \n",
    "# and 0 for ‘No Diabetes’. Use these values as labels for classification (step 4).\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "\n",
    "# Split data into test and training sets (consider 20% for test).\n",
    "# Use PCA on the training data to create 3 new components \n",
    "# from existing features (all columns except outcome).\n",
    "# Transfer training and test data to the new dimensions (PCs).\n",
    "\n",
    "# Step 4: Classification using a super learner\n",
    "\n",
    "# Define three classification models as base classifiers \n",
    "# consisting of Naïve Bayes, Neural Network, and KNN.\n",
    "# Define a decision tree as the meta learner.\n",
    "# Train decision tree (meta learner) on outputs of three base classifiers \n",
    "# using 5-fold cross validation.\n",
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "# Report accuracy of the model on the test data.\n",
    "\n",
    "# Step 5: Employing the model on other datasets\n",
    "\n",
    "# Use the last column of the assigned dataset as outcome (label).\n",
    "# Use your current code for steps 1,3, and 4 \n",
    "# with minor changes (e.g., encoding categorical variables) \n",
    "# to train your model on the new dataset and calculate the accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_csv('diabetes_project.csv')\n",
    "# print(data.head(5))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4f46e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness   BMI  \\\n",
      "0            6    148.0             72           35.0  33.6   \n",
      "1            1     85.0             66           29.0  26.6   \n",
      "2            8      NaN             64          -35.0  23.3   \n",
      "3            1     89.0             66           23.0  28.1   \n",
      "4            0    137.0             40           35.0  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  \n",
      "0                     0.627   50  \n",
      "1                     0.351   31  \n",
      "2                     0.672   32  \n",
      "3                     0.167   21  \n",
      "4                       NaN   33  \n",
      "Pregnancies                   0\n",
      "Glucose                     100\n",
      "BloodPressure                 0\n",
      "SkinThickness               190\n",
      "BMI                           7\n",
      "DiabetesPedigreeFunction     44\n",
      "Age                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check the information of the data\n",
    "print(data.head(5))\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97dd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data pre-processing phase\n",
    "# Remove outliers from all columns.\n",
    "# Impute missing values in all columns.\n",
    "# Normalize all columns.\n",
    "# I think for the four missing values, we should use median values to impute it at first,then remove the outliers.\n",
    "# But I still confused that whether I should remove outliers firstly.\n",
    "\n",
    "# Impute missing values in all columns.\n",
    "# comlumns_to_impute = ['Glucose', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction']\n",
    "# medians = data[comlumns_to_impute].median()\n",
    "# print(\"medians is:\")\n",
    "# print(medians)\n",
    "# data[comlumns_to_impute] = data[comlumns_to_impute].fillna(medians)\n",
    "# print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e67db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                   4\n",
      "Glucose                     100\n",
      "BloodPressure                 0\n",
      "SkinThickness               194\n",
      "BMI                          11\n",
      "DiabetesPedigreeFunction     44\n",
      "Age                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers from all columns.\n",
    "# but the age need to be removed outliers?\n",
    "data_no_outliers = data.copy()\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "for col in features:\n",
    "    Q1 = data_no_outliers[col].quantile(0.25)\n",
    "    Q3 = data_no_outliers[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - (1.5 * IQR)\n",
    "    upper_bound = Q3 + (1.5 * IQR)\n",
    "    data_no_outliers.loc[\n",
    "        (data_no_outliers[col] < lower_bound) | \n",
    "        (data_no_outliers[col] > upper_bound),\n",
    "        col\n",
    "    ] = np.nan\n",
    "print(data_no_outliers.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54324e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medians is:\n",
      "Pregnancies                   3.000\n",
      "Glucose                     117.000\n",
      "BloodPressure                72.000\n",
      "SkinThickness                29.000\n",
      "BMI                          32.150\n",
      "DiabetesPedigreeFunction      0.355\n",
      "Age                          29.000\n",
      "dtype: float64\n",
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values in all columns.\n",
    "comlumns_to_impute = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "medians = data_no_outliers[comlumns_to_impute].median()\n",
    "print(\"medians is:\")\n",
    "print(medians)\n",
    "data_no_outliers[comlumns_to_impute] = data_no_outliers[comlumns_to_impute].fillna(medians)\n",
    "print(data_no_outliers.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608e03a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67961989  0.95479867 -0.03101107  0.69146549  0.21297848  0.96547681\n",
      "   1.42845618]\n",
      " [-0.85791523 -1.26980516 -0.53614696 -0.00522757 -0.87790504 -0.27797363\n",
      "  -0.19807552]\n",
      " [ 1.29463394 -0.13984766 -0.70452559 -0.00522757 -1.3921787   1.16821329\n",
      "  -0.11246859]\n",
      " [-0.85791523 -1.12856047 -0.53614696 -0.70192064 -0.64414429 -1.10694059\n",
      "  -1.05414483]\n",
      " [-1.16542226  0.56637578 -2.72506914  0.69146549  1.69346325 -0.25995261\n",
      "  -0.02686166]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize all columns.\n",
    "scaler = StandardScaler()\n",
    "data_scaler = scaler.fit_transform(data_no_outliers)\n",
    "print(data_scaler[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5502041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Glucose        BMI        Age\n",
      "Cluster                                  \n",
      "0        141.945578  35.415476  42.142857\n",
      "1        106.906606  30.102278  27.400911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Unsupervised Learning for generating labels\n",
    "\n",
    "# Use K-means clustering on three features of Glucose, \n",
    "# BMI and Age to cluster data into two clusters.\n",
    "# Assign ‘Diabetes’ name to the cluster with higher average Glucose \n",
    "# and ‘No Diabetes’ to the other cluster.\n",
    "# Add a new column (Outcome) to the dataset containing 1 for ‘Diabetes’ \n",
    "# and 0 for ‘No Diabetes’. Use these values as labels for classification (step 4).\n",
    "\n",
    "# Use K-means clustering on three features of Glucose, \n",
    "# BMI and Age to cluster data into two clusters.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_features = ['Glucose', 'BMI', 'Age']\n",
    "data_for_kmeans = data_no_outliers[kmeans_features]\n",
    "kmeans_scaler = StandardScaler()\n",
    "data_kmeans_scaled = kmeans_scaler.fit_transform(data_for_kmeans)\n",
    "\n",
    "chose_k = 2\n",
    "kmeans = KMeans(n_clusters = chose_k, init = 'k-means++',n_init = 'auto', random_state = 42)\n",
    "kmeans.fit(data_kmeans_scaled)\n",
    "cluster_labels = kmeans.labels_\n",
    "# come back to the original data to analysis\n",
    "data_no_outliers['Cluster'] = cluster_labels\n",
    "cluster_analysis = data_no_outliers.groupby('Cluster')[kmeans_features].mean()\n",
    "print(cluster_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc527fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes with high average Glucose cluster is: Cluster 0\n",
      "no diabetes with high average Glucose cluster is: Cluster 1\n",
      "   Cluster  Outcome\n",
      "0        0        1\n",
      "1        1        0\n",
      "2        1        0\n",
      "3        1        0\n",
      "4        0        1\n",
      "5        1        0\n",
      "6        1        0\n",
      "7        0        1\n",
      "8        0        1\n",
      "9        1        0\n"
     ]
    }
   ],
   "source": [
    "# Assign ‘Diabetes’ name to the cluster with higher average Glucose \n",
    "# and ‘No Diabetes’ to the other cluster.\n",
    "# Add a new column (Outcome) to the dataset containing 1 for ‘Diabetes’ \n",
    "# and 0 for ‘No Diabetes’. Use these values as labels for classification (step 4).\n",
    "diabetes_cluster_labels = cluster_analysis['Glucose'].idxmax()\n",
    "no_diabetes_cluster_labels = 1 - diabetes_cluster_labels\n",
    "print(f\"diabetes with high average Glucose cluster is: Cluster {diabetes_cluster_labels}\")\n",
    "print(f\"no diabetes with high average Glucose cluster is: Cluster {no_diabetes_cluster_labels}\")\n",
    "# we use Map to distribute the labels\n",
    "label_map = {\n",
    "    diabetes_cluster_labels: 1,  # 1 = 'Diabetes'\n",
    "    no_diabetes_cluster_labels: 0   # 0 = 'No Diabetes'\n",
    "}\n",
    "data_no_outliers['Outcome'] = data_no_outliers['Cluster'].map(label_map)\n",
    "print(data_no_outliers[['Cluster','Outcome']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64df36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original features: (586, 7)\n",
      "PCA train features: (586, 3)\n",
      "PCA test features: (147, 3)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Extraction\n",
    "\n",
    "# Split data into test and training sets (consider 20% for test).\n",
    "# Use PCA on the training data to create 3 new components \n",
    "# from existing features (all columns except outcome).\n",
    "# Transfer training and test data to the new dimensions (PCs).\n",
    "\n",
    "# ! data_no_outliers is a Pandas DataFrame.\n",
    "# ! data_scaler is a NumPy Array.\n",
    "# so we need to normalize it again\n",
    "\n",
    "from psutil import net_connections\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "target = 'Outcome'\n",
    "\n",
    "X = data_no_outliers[features]\n",
    "y = data_no_outliers[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "PCA_scaler = StandardScaler()\n",
    "X_train_scaled = PCA_scaler.fit_transform(X_train)\n",
    "X_test_scaled = PCA_scaler.transform(X_test)\n",
    "\n",
    "# Use PCA on the training data to create 3 new components from existing features (all columns except outcome).\n",
    "pca = PCA(n_components = 3)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"original features: {X_train_scaled.shape}\")\n",
    "print(f\"PCA train features: {X_train_pca.shape}\")\n",
    "print(f\"PCA test features: {X_test_pca.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf97a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Classification using a super learner\n",
    "\n",
    "# Define three classification models as base classifiers \n",
    "# consisting of Naïve Bayes, Neural Network, and KNN.\n",
    "# Define a decision tree as the meta learner.\n",
    "# Train decision tree (meta learner) on outputs of three base classifiers \n",
    "# using 5-fold cross validation.\n",
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "# Report accuracy of the model on the test data.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# create instance for three classification models\n",
    "base_nb = GaussianNB()\n",
    "base_nn = MLPClassifier(max_iter=1000, random_state=42)\n",
    "base_knn = KNeighborsClassifier()\n",
    "# create the meta learner\n",
    "meta_learner = DecisionTreeClassifier(random_state=42)\n",
    "level0_estimators = [\n",
    "    ('nb', base_nb),\n",
    "    ('nn', base_nn),\n",
    "    ('knn', base_knn)\n",
    "]\n",
    "#create super learner using 5-fold cross validation\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=level0_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620ed3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m      5\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# try more hyperparemeters\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn__n_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_estimator__min_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m     17\u001b[0m }\n\u001b[0;32m     19\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     20\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mstacking_model, \n\u001b[0;32m     21\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train_pca, y_train)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest hyperparameters: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mbest score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1046\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1605\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    993\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    994\u001b[0m         )\n\u001b[0;32m    995\u001b[0m     )\n\u001b[1;32m--> 997\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    998\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    999\u001b[0m         clone(base_estimator),\n\u001b[0;32m   1000\u001b[0m         X,\n\u001b[0;32m   1001\u001b[0m         y,\n\u001b[0;32m   1002\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m   1003\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m   1004\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m   1005\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m   1006\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     73\u001b[0m warning_filters \u001b[38;5;241m=\u001b[39m warnings\u001b[38;5;241m.\u001b[39mfilters\n\u001b[0;32m     74\u001b[0m iterable_with_config_and_warning_filters \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     (\n\u001b[0;32m     76\u001b[0m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config_and_warning_filters)\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32md:\\pythonWorkplace\\anaconda\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "\n",
    "# we use GridSearchCV to find the best hyperparameters\n",
    "# for the naive bayes,we don't need to change the hyperparameters\n",
    "param_grid = {\n",
    "    # try more hyperparemeters\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "\n",
    "    # try more layer sizes and learning rate\n",
    "    'nn__hidden_layer_sizes': [(25,), (50,), (100,), (25, 25)],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01],\n",
    "\n",
    "    # try more hyperparemeters\n",
    "    'final_estimator__max_depth': [3, 5, 7, 10],\n",
    "    'final_estimator__min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=stacking_model, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "print(f\"best hyperparameters: \\n{grid_search.best_params_}\")\n",
    "print(f\"\\nbest score: {grid_search.best_score_:.4f}\")\n",
    "# output\n",
    "# best hyperparameters:  {'final_estimator__max_depth': 3, \n",
    "#                         'final_estimator__min_samples_leaf': 5, \n",
    "#                         'knn__n_neighbors': 5, \n",
    "#                         'knn__weights': 'uniform', \n",
    "#                         'nn__alpha': 0.0001, \n",
    "#                         'nn__hidden_layer_sizes': (50,)} \n",
    "# best score: 0.8772\n",
    "# Accuracy: 0.8707\n",
    "\n",
    "# Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.83      0.95      0.89        78\n",
    "#            1       0.93      0.78      0.85        69\n",
    "\n",
    "#     accuracy                           0.87       147\n",
    "#    macro avg       0.88      0.87      0.87       147\n",
    "# weighted avg       0.88      0.87      0.87       147\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81684ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8707\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89        78\n",
      "           1       0.93      0.78      0.85        69\n",
      "\n",
      "    accuracy                           0.87       147\n",
      "   macro avg       0.88      0.87      0.87       147\n",
      "weighted avg       0.88      0.87      0.87       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report accuracy of the model on the test data.\n",
    "y_pred_test = grid_search.predict(X_test_pca)\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "final_report = classification_report(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ee653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Employing the model on other datasets\n",
    "\n",
    "# Use the last column of the assigned dataset as outcome (label).\n",
    "# Use your current code for steps 1,3, and 4 \n",
    "# with minor changes (e.g., encoding categorical variables) \n",
    "# to train your model on the new dataset and calculate the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
