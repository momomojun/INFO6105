{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7892340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2927 entries, 0 to 2926\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              2927 non-null   int64  \n",
      " 1   education        2927 non-null   int64  \n",
      " 2   sex              2927 non-null   object \n",
      " 3   is_smoking       2927 non-null   object \n",
      " 4   cigsPerDay       2927 non-null   int64  \n",
      " 5   BPMeds           2927 non-null   int64  \n",
      " 6   prevalentStroke  2927 non-null   int64  \n",
      " 7   prevalentHyp     2927 non-null   int64  \n",
      " 8   diabetes         2927 non-null   int64  \n",
      " 9   totChol          2927 non-null   int64  \n",
      " 10  sysBP            2927 non-null   float64\n",
      " 11  diaBP            2927 non-null   float64\n",
      " 12  BMI              2927 non-null   float64\n",
      " 13  heartRate        2927 non-null   int64  \n",
      " 14  glucose          2927 non-null   int64  \n",
      " 15  TenYearCHD       2927 non-null   int64  \n",
      "dtypes: float64(3), int64(11), object(2)\n",
      "memory usage: 366.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "\n",
    "# Developing a semi-supervised learning model on Diabetes dataset \n",
    "# using a super learner and deploying the model on other datasets.\n",
    "\n",
    "# Step 1: Data pre-processing phase\n",
    "\n",
    "# Remove outliers from all columns.\n",
    "# Impute missing values in all columns.\n",
    "# Normalize all columns.\n",
    "\n",
    "# Step 2: Unsupervised Learning for generating labels\n",
    "\n",
    "# Use K-means clustering on three features of Glucose, \n",
    "# BMI and Age to cluster data into two clusters.\n",
    "# Assign ‘Diabetes’ name to the cluster with higher average Glucose \n",
    "# and ‘No Diabetes’ to the other cluster.\n",
    "# Add a new column (Outcome) to the dataset containing 1 for ‘Diabetes’ \n",
    "# and 0 for ‘No Diabetes’. Use these values as labels for classification (step 4).\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "\n",
    "# Split data into test and training sets (consider 20% for test).\n",
    "# Use PCA on the training data to create 3 new components \n",
    "# from existing features (all columns except outcome).\n",
    "# Transfer training and test data to the new dimensions (PCs).\n",
    "\n",
    "# Step 4: Classification using a super learner\n",
    "\n",
    "# Define three classification models as base classifiers \n",
    "# consisting of Naïve Bayes, Neural Network, and KNN.\n",
    "# Define a decision tree as the meta learner.\n",
    "# Train decision tree (meta learner) on outputs of three base classifiers \n",
    "# using 5-fold cross validation.\n",
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "# Report accuracy of the model on the test data.\n",
    "\n",
    "# Step 5: Employing the model on other datasets\n",
    "\n",
    "# Use the last column of the assigned dataset as outcome (label).\n",
    "# Use your current code for steps 1,3, and 4 \n",
    "# with minor changes (e.g., encoding categorical variables) \n",
    "# to train your model on the new dataset and calculate the accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_csv('Datasets/10year_risk_coronary_heart_disease.csv')\n",
    "# print(data.head(5))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae4564db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2927 entries, 0 to 2926\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              2927 non-null   int64  \n",
      " 1   education        2927 non-null   int64  \n",
      " 2   cigsPerDay       2927 non-null   int64  \n",
      " 3   BPMeds           2927 non-null   int64  \n",
      " 4   prevalentStroke  2927 non-null   int64  \n",
      " 5   prevalentHyp     2927 non-null   int64  \n",
      " 6   diabetes         2927 non-null   int64  \n",
      " 7   totChol          2927 non-null   int64  \n",
      " 8   sysBP            2927 non-null   float64\n",
      " 9   diaBP            2927 non-null   float64\n",
      " 10  BMI              2927 non-null   float64\n",
      " 11  heartRate        2927 non-null   int64  \n",
      " 12  glucose          2927 non-null   int64  \n",
      " 13  TenYearCHD       2927 non-null   int64  \n",
      " 14  sex_M            2927 non-null   bool   \n",
      " 15  is_smoking_YES   2927 non-null   bool   \n",
      "dtypes: bool(2), float64(3), int64(11)\n",
      "memory usage: 326.0 KB\n",
      "   age  education  cigsPerDay  BPMeds  prevalentStroke  prevalentHyp  \\\n",
      "0   36          4           0       0                0             1   \n",
      "1   46          1          10       0                0             0   \n",
      "2   50          1          20       0                0             1   \n",
      "3   64          1          30       0                0             0   \n",
      "4   61          3           0       0                0             1   \n",
      "\n",
      "   diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  TenYearCHD  \\\n",
      "0         0      212  168.0   98.0  29.77         72       75           0   \n",
      "1         0      250  116.0   71.0  20.35         88       94           0   \n",
      "2         0      233  158.0   88.0  28.26         68       94           1   \n",
      "3         0      241  136.5   85.0  26.42         70       77           0   \n",
      "4         0      272  182.0  121.0  32.80         85       65           1   \n",
      "\n",
      "   sex_M  is_smoking_YES  \n",
      "0   True           False  \n",
      "1  False            True  \n",
      "2   True            True  \n",
      "3  False            True  \n",
      "4  False           False  \n"
     ]
    }
   ],
   "source": [
    "#pre processing\n",
    "target = 'TenYearCHD'\n",
    "categorical_features = ['sex', 'is_smoking']\n",
    "data_encoded = pd.get_dummies(data, \n",
    "                                  columns=categorical_features, \n",
    "                                  drop_first=True)\n",
    "data_encoded.info()\n",
    "print(data_encoded.head())\n",
    "\n",
    "# because the data is non-null, so we don't need to use IQR to fill the Datasets\n",
    "# what we need to do is normalize the dataset in the next code cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18fa15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original features: (2341, 15)\n",
      "PCA train features: (2341, 3)\n",
      "PCA test features: (586, 3)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Extraction\n",
    "\n",
    "# Split data into test and training sets (consider 20% for test).\n",
    "# Use PCA on the training data to create 3 new components \n",
    "# from existing features (all columns except outcome).\n",
    "# Transfer training and test data to the new dimensions (PCs).\n",
    "\n",
    "from psutil import net_connections\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "features = ['age','education','cigsPerDay','BPMeds','prevalentStroke',\n",
    "            'prevalentHyp','diabetes','totChol','sysBP','diaBP','BMI',\n",
    "            'heartRate','glucose','sex_M','is_smoking_YES']\n",
    "\n",
    "X = data_encoded[features]\n",
    "y = data_encoded[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "PCA_scaler = StandardScaler()\n",
    "X_train_scaled = PCA_scaler.fit_transform(X_train)\n",
    "X_test_scaled = PCA_scaler.transform(X_test)\n",
    "\n",
    "# Use PCA on the training data to create 3 new components from existing features (all columns except outcome).\n",
    "pca = PCA(n_components = 3)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(f\"original features: {X_train_scaled.shape}\")\n",
    "print(f\"PCA train features: {X_train_pca.shape}\")\n",
    "print(f\"PCA test features: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aed47ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters: \n",
      "{'final_estimator__max_depth': 3, 'final_estimator__min_samples_leaf': 10, 'knn__n_neighbors': 9, 'knn__weights': 'uniform', 'nn__alpha': 0.01, 'nn__hidden_layer_sizes': (100,)}\n",
      "\n",
      "best score: 0.8513\n",
      "Accuracy: 0.8430\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91       496\n",
      "           1       0.25      0.01      0.02        90\n",
      "\n",
      "    accuracy                           0.84       586\n",
      "   macro avg       0.55      0.50      0.47       586\n",
      "weighted avg       0.76      0.84      0.78       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Classification using a super learner\n",
    "\n",
    "# Define three classification models as base classifiers \n",
    "# consisting of Naïve Bayes, Neural Network, and KNN.\n",
    "# Define a decision tree as the meta learner.\n",
    "# Train decision tree (meta learner) on outputs of three base classifiers \n",
    "# using 5-fold cross validation.\n",
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "# Report accuracy of the model on the test data.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# create instance for three classification models\n",
    "base_nb = GaussianNB()\n",
    "base_nn = MLPClassifier(max_iter=1000, random_state=42)\n",
    "base_knn = KNeighborsClassifier()\n",
    "# create the meta learner\n",
    "meta_learner = DecisionTreeClassifier(random_state=42)\n",
    "level0_estimators = [\n",
    "    ('nb', base_nb),\n",
    "    ('nn', base_nn),\n",
    "    ('knn', base_knn)\n",
    "]\n",
    "#create super learner using 5-fold cross validation\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=level0_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv = 5\n",
    ")\n",
    "param_grid = {\n",
    "    # try more hyperparemeters\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "\n",
    "    # try more layer sizes and learning rate\n",
    "    'nn__hidden_layer_sizes': [(25,), (50,), (100,), (25, 25)],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01],\n",
    "\n",
    "    # try more hyperparemeters\n",
    "    'final_estimator__max_depth': [3, 5, 7, 10],\n",
    "    'final_estimator__min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=stacking_model, \n",
    "    param_grid=param_grid, \n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "print(f\"best hyperparameters: \\n{grid_search.best_params_}\")\n",
    "print(f\"\\nbest score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Report accuracy of the model on the test data.\n",
    "y_pred_test = grid_search.predict(X_test_pca)\n",
    "\n",
    "final_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "final_report = classification_report(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(final_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
