{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7892340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2927 entries, 0 to 2926\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   age              2927 non-null   int64  \n",
      " 1   education        2927 non-null   int64  \n",
      " 2   sex              2927 non-null   object \n",
      " 3   is_smoking       2927 non-null   object \n",
      " 4   cigsPerDay       2927 non-null   int64  \n",
      " 5   BPMeds           2927 non-null   int64  \n",
      " 6   prevalentStroke  2927 non-null   int64  \n",
      " 7   prevalentHyp     2927 non-null   int64  \n",
      " 8   diabetes         2927 non-null   int64  \n",
      " 9   totChol          2927 non-null   int64  \n",
      " 10  sysBP            2927 non-null   float64\n",
      " 11  diaBP            2927 non-null   float64\n",
      " 12  BMI              2927 non-null   float64\n",
      " 13  heartRate        2927 non-null   int64  \n",
      " 14  glucose          2927 non-null   int64  \n",
      " 15  TenYearCHD       2927 non-null   int64  \n",
      "dtypes: float64(3), int64(11), object(2)\n",
      "memory usage: 366.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Summary:\n",
    "\n",
    "# Developing a semi-supervised learning model on Diabetes dataset \n",
    "# using a super learner and deploying the model on other datasets.\n",
    "\n",
    "# Step 1: Data pre-processing phase\n",
    "\n",
    "# Remove outliers from all columns.\n",
    "# Impute missing values in all columns.\n",
    "# Normalize all columns.\n",
    "\n",
    "# Step 2: Unsupervised Learning for generating labels\n",
    "\n",
    "# Use K-means clustering on three features of Glucose, \n",
    "# BMI and Age to cluster data into two clusters.\n",
    "# Assign ‘Diabetes’ name to the cluster with higher average Glucose \n",
    "# and ‘No Diabetes’ to the other cluster.\n",
    "# Add a new column (Outcome) to the dataset containing 1 for ‘Diabetes’ \n",
    "# and 0 for ‘No Diabetes’. Use these values as labels for classification (step 4).\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "\n",
    "# Split data into test and training sets (consider 20% for test).\n",
    "# Use PCA on the training data to create 3 new components \n",
    "# from existing features (all columns except outcome).\n",
    "# Transfer training and test data to the new dimensions (PCs).\n",
    "\n",
    "# Step 4: Classification using a super learner\n",
    "\n",
    "# Define three classification models as base classifiers \n",
    "# consisting of Naïve Bayes, Neural Network, and KNN.\n",
    "# Define a decision tree as the meta learner.\n",
    "# Train decision tree (meta learner) on outputs of three base classifiers \n",
    "# using 5-fold cross validation.\n",
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "# Report accuracy of the model on the test data.\n",
    "\n",
    "# Step 5: Employing the model on other datasets\n",
    "\n",
    "# Use the last column of the assigned dataset as outcome (label).\n",
    "# Use your current code for steps 1,3, and 4 \n",
    "# with minor changes (e.g., encoding categorical variables) \n",
    "# to train your model on the new dataset and calculate the accuracy.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "data = pd.read_csv('Datasets/10year_risk_coronary_heart_disease.csv')\n",
    "# print(data.head(5))\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4564db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                0\n",
      "education          0\n",
      "cigsPerDay         0\n",
      "BPMeds             0\n",
      "prevalentStroke    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#pre processing\n",
    "target = 'TenYearCHD'\n",
    "y_new = data[target]\n",
    "X_new = data.drop(columns=[target])\n",
    "categorical_features = ['sex', 'is_smoking']\n",
    "X_new_encoded = pd.get_dummies(X_new, columns=categorical_features, drop_first=True)\n",
    "\n",
    "categorical_features_encoded = ['sex_M', 'is_smoking_YES']\n",
    "continuous_features = [\n",
    "    col for col in X_new_encoded.columns \n",
    "    if col not in categorical_features_encoded\n",
    "]\n",
    "data_processed = X_new_encoded.copy()\n",
    "\n",
    "for col in continuous_features:\n",
    "    Q1 = data_processed[col].quantile(0.25)\n",
    "    Q3 = data_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - (1.5 * IQR)\n",
    "    upper_bound = Q3 + (1.5 * IQR)\n",
    "    data_processed.loc[\n",
    "        (data_processed[col] < lower_bound) | \n",
    "        (data_processed[col] > upper_bound),\n",
    "        col\n",
    "    ] = np.nan\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_processed[continuous_features] = imputer.fit_transform(data_processed[continuous_features])\n",
    "print(data_processed[continuous_features].isnull().sum().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fa15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Extraction\n",
    "\n",
    "# Split data into test and training sets (consider 20% for test).\n",
    "# Use PCA on the training data to create 3 new components \n",
    "# from existing features (all columns except outcome).\n",
    "# Transfer training and test data to the new dimensions (PCs).\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_processed, y_new, test_size=0.2, random_state=42)\n",
    "PCA_scaler = StandardScaler()\n",
    "X_train_scaled = PCA_scaler.fit_transform(X_train)\n",
    "X_test_scaled = PCA_scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed47ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters: \n",
      "{'superlearner__final_estimator__max_depth': 7, 'superlearner__final_estimator__min_samples_leaf': 10, 'superlearner__knn__n_neighbors': 9, 'superlearner__knn__weights': 'distance', 'superlearner__nn__alpha': 0.0001, 'superlearner__nn__hidden_layer_sizes': (100,)}\n",
      "\n",
      "Classification Report (SMOTE):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83       496\n",
      "           1       0.21      0.30      0.25        90\n",
      "\n",
      "    accuracy                           0.72       586\n",
      "   macro avg       0.54      0.55      0.54       586\n",
      "weighted avg       0.76      0.72      0.74       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Classification using a super learner\n",
    "\n",
    "# Define three classification models as base classifiers \n",
    "# consisting of Naïve Bayes, Neural Network, and KNN.\n",
    "# Define a decision tree as the meta learner.\n",
    "# Train decision tree (meta learner) on outputs of three base classifiers \n",
    "# using 5-fold cross validation.\n",
    "# Find hyperparameters for all these models which provide the best accuracy rate.\n",
    "# Report accuracy of the model on the test data.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# create instance for three classification models\n",
    "base_nb = GaussianNB()\n",
    "base_nn = MLPClassifier(max_iter=1000, random_state=42)\n",
    "base_knn = KNeighborsClassifier()\n",
    "# create the meta learner\n",
    "meta_learner = DecisionTreeClassifier(random_state=42)\n",
    "level0_estimators = [\n",
    "    ('nb', base_nb),\n",
    "    ('nn', base_nn),\n",
    "    ('knn', base_knn)\n",
    "]\n",
    "#create super learner using 5-fold cross validation\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=level0_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv = 5\n",
    ")\n",
    "smote_pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('superlearner', stacking_model) \n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # try more hyperparemeters\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "\n",
    "    # try more layer sizes and learning rate\n",
    "    'nn__hidden_layer_sizes': [(25,), (50,), (100,), (25, 25)],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01],\n",
    "\n",
    "    # try more hyperparemeters\n",
    "    'final_estimator__max_depth': [3, 5, 7, 10],\n",
    "    'final_estimator__min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# create SMOTE param_grid\n",
    "smote_param_grid = {}\n",
    "for key, value in param_grid.items():\n",
    "    smote_param_grid[f'superlearner__{key}'] = value\n",
    "\n",
    "# change GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=smote_pipeline,      # use SMOTE Pipeline\n",
    "    param_grid=smote_param_grid,   # use SMOTE grid\n",
    "    cv=5, \n",
    "    scoring='f1_macro',            # don't use 'accuracy', use F1-score\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "print(f\"best hyperparameters: \\n{grid_search.best_params_}\")\n",
    "y_pred_test = grid_search.predict(X_test_pca)\n",
    "print(f\"\\nbest score: {grid_search.best_score_:.4f}\")\n",
    "y_pred_test = grid_search.predict(X_test_pca)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "final_report = classification_report(y_test, y_pred_test)\n",
    "print(\"\\nClassification Report (SMOTE):\")\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f54849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best score: 0.5609\n",
      "Accuracy: 0.7235\n"
     ]
    }
   ],
   "source": [
    "# print(f\"\\nbest score: {grid_search.best_score_:.4f}\")\n",
    "# y_pred_test = grid_search.predict(X_test_pca)\n",
    "\n",
    "# final_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "# print(f\"Accuracy: {final_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
